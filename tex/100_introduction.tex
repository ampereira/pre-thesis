%!TEX root = ../main.tex

\chapter{Introduction}
\label{introduction}

Today's computing platforms are becoming increasingly complex with multiple interconnected computing nodes, each with multiple multicore CPU chips, and sometimes coupled with hardware accelerators. While the application performance is an important issue to tackle, the efficient usage of the resources of these systems is a crucial subject that needs to be addressed. Guaranteeing that the available computational resources are being adequately used by an application may require deep knowledge of the underlying architecture details of both CPUs and hardware accelerators, as well as extensive tuning of each individual application. It is crucial to comprehend the key issues that currently have more impact on computing performance and efficiency, namely the relationship between the cost of numerical computation, memory access, and data communication among available computing units. The architecture design of many-core hardware accelerators is significantly different among devices, with no standard yet defined. The programmer must know the architectural details of each hardware accelerator and their interconnection topology to the CPU to produce efficient code.

From the hardware point of view, efficiency may have a different meaning: it can be considered as the ratio between power usage and computational throughput. This is a subject of extensive research in a field also known as ``Green Computing'', where the goal is to reduce power consumption of the hardware while minimising the performance degradation. This is important for both mobile computing and to reduce the cost of maintaining huge computing clusters and data centres.

Computing clusters are the most popular High Performance Computing (HPC) platforms, constituted of many different computing nodes, interconnected by specialised communication channels in a distributed memory environment. The computing nodes may be characterised as homogeneous or heterogeneous platforms, where the former has one or more CPUs in a shared memory environment, and the latter has hardware accelerators coupled to the CPUs by a PCI-Express interface, in a distributed memory environment. This implies that the data is always visible to the CPUs, but must be explicitly transferred to the accelerator devices.

A proper data management is relevant to ensure the efficiency of an application. Code parallelism is a must to take advantage of the multiple cores in both the CPUs and the hardware accelerators, adapted to the different memory and programming paradigms. Data races, resource contention and, when considering heterogeneous platforms, explicit memory transfers are complex challenges for the programmer. Also, each accelerator manufacturer uses their own frameworks and compilers to program their devices. Non-computer scientists opt to invest most of the research time on their field of science and have little time to improve their programming skills. These factors reinforced the collaboration of multidisciplinary teams of scientists from various fields with computer scientists to develop high performing, efficient, and robust applications.

\section{Context}
\label{context}

The European Organization for Nuclear Research \cite{CERN} (CERN, acronym for \textit{Conseil Européen pour la Recherche Nucléaire}) is a consortium of 21 European countries and more than 30 ``observer'' countries, with the purpose of operating the largest particle physics laboratory in the world. Founded in 1954, CERN is located in the border between France and Switzerland, and employs thousands of scientists and engineers representing 608 universities and research groups of 113 different nationalities.

CERN research focus on the basic constituents of matter to understand the fundamental structure of the universe, which started by studying the atomic nucleus but quickly progressed into high energy physics (HEP), namely on the interactions between particles. The instrumentation used in nuclear and particle physics research is essentially formed by particle accelerators and detectors, alongside with the facilities necessary for delivering the protons to the accelerators. The Large Hadron Collider (LHC) particle accelerator (later presented) speeds up groups of particles close to the speed of light, in opposite directions, inducing a controlled collision at the detectors core (the collision of two particles is referred as an ``event''). The detectors record various characteristics of the resultant particles of each collision, such as energy and momentum, which originate from complex decay processes of the original particles. The purpose of these experiments is to test models and predictions in High Energy Physics (HEP), such as the Standard Model, by confirming or discovering new particles and interactions.

CERN started with a small low energy particle accelerator, the Proton Synchrotron \cite{CERN:PS} inaugurated in 1959, but soon its equipment was iteratively upgraded and expanded. The current facilities are constituted by the older accelerators (some already decommissioned) and particle detectors, as well as the newer Large Hadron Collider (LHC) \cite{CERN:LHC} high energy particle accelerator, located 100 meter underground and with a 27 km circumference length. There are currently seven experiments running on the LHC: CMS \cite{CERN:CMS}, ATLAS \cite{CERN:ATLAS}, LHCb \cite{CERN:LHCb}, MoEDAL \cite{CERN:MoEDAL}, TOTEM \cite{CERN:TOTEM}, LHC-forward \cite{CERN:LHCf} and ALICE \cite{CERN:ALICE}. Each of these experiments have their own detector on the LHC and conduct HEP analysis, using distinct technologies and research approaches. One of the most relevant researches being conducted at CERN is the validation of the Standard Model and discovery of the Higgs boson theory. The ATLAS experiment, a key project at CERN, aims to study the properties of the recently discovered Higgs boson \cite{Higgs}, the search for new particles predicted by models of physics beyond the Standard Model like Susy, searches for new heavy gauge bosons and precision measurements where the top quark is of utmost importance. During the next year the LHC will be upgraded to increase its luminosity, e.g., the amount of energy of the accelerated particle beams.

Approximately 600 millions of collisions occur every second at the LHC. Particles produced in head-on proton collisions interact with the detectors of the ATLAS experiment, generating massive amounts of raw data as electric signals. It is estimated that all the detectors combined produce 25 petabytes of data per year \cite{CERN:DATA1,CERN:DATA2}. CERN does not have the financial resources to afford the computational power necessary to process all data from the detectors, which motivated the creation of the Worldwide LHC Computing Grid \cite{CERN:WLHCCG}, a distributed computing infrastructure that uses the resources of the scientific community for data processing. The grid is organized in a hierarchy divided in 4 tiers. Each tier is made by one or more computing centres and has a set of specific tasks and services to perform, such as store, filter, refine and analyse all the data gathered at the LHC.

The Tier-0 is the data centre located at CERN. It provides 20\% of the total grid computing capacity, and its goal is to store and reconstruct the raw data gathered at the detectors in the LHC, converting it into meaningful information, to be used by the remaining tiers. The data is received on a format designed for this reconstruction, with information about the event, detector and software diagnostics. The output of the reconstruction has two formats, the Event Summary Data (ESD) and the Analysis Object Data (AOD), each with different purposes, containing information of the reconstructed objects and calibration parameters, which can be used for early analysis. This tier distributes the raw data and the reconstructed output by the 11 Tier-1 computational centres, spread among the different member countries of CERN.

Tier-1 computational centres are responsible for storing a portion of the raw and reconstructed data and provide support to the grid. In this tier, the reconstructed data suffers more processing and refinement, to filter the relevant information and reduce its size, which is now in Derived Physics Data (DPD) format, to be then transferred to the Tier-2 computational centres. The size of the data for an event is reduced from 3 MB (raw) to 10 kB (DPD). This tier also stores the output of the simulations performed at Tier-2. The Tier-0 centre is connected to the 11 Tier-1 centres by high bandwidth optical fiber links, which form the LHC Optical Private Network.

There are roughly 140 Tier-2 computational centres spread around the world. Their main purpose is to perform both Monte-Carlo simulations and a portion of the events reconstructions, with the data received from the Tier-1 centres. The Tier-3 centres range from university clusters to desktop computers, and they are responsible for most events reconstruction and final data analysis. In the CERN terminology, an application that is designed to process a given amount of data to extract relevant physics information about events, which may support a specific HEP theory, is called an analysis.

The Laboratório de Instrumentação e Física Experimental de Partículas (LIP) \cite{LIP} is a portuguese scientific and technical association for research on experimental high energy physics and associated instrumentation. LIP has a strong collaboration with CERN as it was the first scientific organisation from Portugal that joined CERN, in 1986. It has laboratories in Lisbon, Coimbra and Minho and 170 people employed. LIP researchers have produced several applications for testing at ATLAS several HEP theoretical models that use Tier-2 and Tier-3 computational resources for data analysis. Most of the analysis applications use in-house developed skeleton libraries, such as the LipCbrAnalysis and LipMiniAnalysis.

There is the need to process more data, more accurately, in less time, which often leads to investments on larger computing clusters to improve the quality of the research results. However, most scientific code was not designed and/or developed for an efficient use of the available computational resources of modern platforms. If these applications were adequately designed (or tuned), the event analysis throughput could be massively increased. An efficient parallel application can significantly improve its performance at a much lower cost \cite{Msc:AMP}.

\section{Problem and Motivation}
\label{motivation}

With an increase in particle collisions and data being produced by the detectors at the LHC, research groups will need a larger budget to acquire and maintain the required computational resources to keep up with the analysis. Moreover, research groups working on the same experiment enforce positive competition to find and publish relevant results. The amount and quality of event processing has a direct impact on the research, meaning that groups with access to the most efficient computational resources become ahead of the competition.

Improving the accuracy of each event analysis benefits the quality of the physics properties being studied. Due to several intrinsic ATLAS experimental effects like energy and transverse momentum resolutions, the measured kinematic properties of particles produced in a collision may be shifted within a range of $\pm1\%$, implying an uncertainty that is propagated through the event analysis. It is possible to improve the reconstruction quality by varying the values measured by the detector within that range, but with a significant impact to the analysis execution time, leading to a trade-off between the event processing throughput and their reconstruction quality.

Scientists at LIP developed the LipCbrAnalysis skeleton library to aid the development of data analysis applications. It contains a set of physics utilities, such as specific classes and functions, and removes the need to code the input file reading, memory allocation of each event data, and output creation for every data analysis application. With this, the programmer may focus on the specifics of the required analysis, such as the filtering and reconstruction of events. An improved version was developed, LipMiniAnalysis, aiming to read a new type of input data files, and stripping the former skeleton of outdated features.

An efficiency study and optimisation of one of LIP production data analysis, also used as a case study for a background research on this issue, was presented in \cite{Msc:AMP,paperAMP}. It tackled the computational inefficiencies of the application on both homogeneous and heterogeneous platforms, and identified several limitations to performance scalability, specially when using hardware accelerators. The data analysis case study and the limitations identified with the LipMiniAnalysis skeleton are presented in a later section.

At the LHC, two proton beams are accelerated close to the speed of light in opposite directions, set to collide inside a specific particle detector. This head-on collision triggers a chain reaction of decaying particles, and most of the final particles interact with the detector, allowing to record relevant data. One of the searches being conducted at the ATLAS Experiment relates to the study of the Higgs boson couplings to top quarks. Figure \ref{fig:ttH} represents the final state topology of the associated production of two top quarks and one Higgs boson (that decays to $b\bar{b}$, two bottom quarks), known as \ttH production. Figure \ref{fig:ttbar} provides a schematic representation of the system to highlight the key features, such as the bottom quarks being jets of smaller particles, and the leptons (both $l^+$ and $l^-$) being a muon and electron in the $t$ and $\bar{t}$ decays, respectively.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[scale=0.3]{imgs/ttH_feynman.png}
		\caption{Feynman diagram of the \ttbar and Higgs boson production.}
		\label{fig:ttH}
	\end{center}
\end{figure}

Neutrinos ($v\bar{v}$) do not interact with the detector, so their characteristics are not recorded. Since the top quark reconstruction requires the neutrinos information, their characteristics are analytically determined with the remaining data, known as kinematical reconstruction. However, the \ttbar system may not have a possible reconstruction: the reconstruction has an intrinsic uncertainty associated which determines its accuracy.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[scale=0.45]{imgs/ttbar_higgs.png}
		\caption{Schematic representation of the \ttbar system and Higgs boson decay.}
		\label{fig:ttbar}
	\end{center}
\end{figure}

The amount of jets from bottom quarks and leptons present in the events may vary according to the decay channel of the $W$ bosons produced in the top quark decays. As shown in figure \ref{fig:ttbar}, four jets and two leptons are required to be present in the events. Two of the jets and two leptons are needed to reconstruct the \ttbar system, and the remaining two jets are used for the Higgs boson reconstruction. For the kinematical reconstruction, every possible combination of jets and leptons must be evaluated and only the most accurate reconstruction is considered. If the $t\bar{t}$ system has a possible solution, the Higgs boson is reconstructed from the jets of the two remaining bottom quarks. The Higgs reconstruction does not use the jets that were associated to the best \ttbar system reconstruction. The overall quality of the event processing depends on the combined accuracy of both reconstructions.

For the global event reconstruction, several solutions can be tested if we assume that the ATLAS detector has an experimental energy-momentum resolution of $\pm1\%$, by varying these quantities within their uncertainty. This uncertainty is propagated into the \ttbar system and Higgs analysis, affecting their accuracy. To improve the quality of the reconstructions several random variations are applied to the measured values, within a maximum range of $|1\%|$ next to the measured values, and apply the process explained previously for each variation. The quality of the event analysis and the application execution time is directly proportional to the amount of variations performed. The goal is to do as many variations as possible within a reasonable time frame.

The current version of \tth, the application developed to perform this analysis, is capable of reconstructing \~3000 events per second on a 2.60 GHz \intel Xeon CPU, without the accuracy improvement. Physicists consider that 1000 variations within the $\pm1\%$ uncertainty would have a big impact on the analysis accuracy, but for \tth is only capable of processing 5 events per second with this precision. The reconstruction of the \ttH system accounts for 99.8\% of \tth execution time with this level of accuracy. To be viable to analyse the \ttH system with 1000 variations it is crucial to improve the \tth efficiency.

\section{Goals and Scientific Contribution}
\label{goals}

Dealing with scientific applications developed by scientists is not trivial due to the code structure and organisation. Several studies \cite{SC:Nature,SC:Develop,SC:SC11,SC:28280} identified the causes that lead scientists to produce poor code:

\begin{center}
	\begin{itemize}
		\item Most scientists are self-taught programmers with inadequate or outdated computer science background.
		\item Scientists disregard software engineering principles to produce efficient code that is also robust, modular, and long lasting.
		\item Scientists often iteratively develop over the same application, producing legacy code (some applications currently in production are iterated on for the last 20 years), and not documenting it so that it can be used by others.
		\item Scientists are seldom aware of profiling and debugging tools, as well as parallelization paradigms.
		\item Scientists cannot afford to get into the architectural details of the newer generations of computing systems, reducing the portability of the code they produce.
	\end{itemize}
\end{center}

To improve the quality of the scientific code, scientists agree that it is crucial to create an interface between their field and computer science by having multidisciplinary teams. However, computer scientists often lack the expert field specific knowledge required to be acknowledged as an integral part of these teams. This often makes scientists sceptical to let others restructure, and even develop from scratch, legacy code that they have been using for years.

The goal of this PhD dissertation is to provide an efficient unified framework for the development of particle physics data analysis applications, designed in close cooperation with the LIP research group. It aims to give an abstraction to the current data analysis programming model, so that the user only codes the sections relative to each specific data analysis, while the framework guarantees portable efficiency for both homogeneous and heterogeneous platforms. The physics researchers will spend less time developing applications, while the framework ensures that the code is automatic parallelized and efficiently uses the computing power of both CPUs and accelerator devices, improving both the data analysis accuracy and throughput.

With a more agile development of high performance data analysis applications, researchers can spend more time improving the algorithms accuracy, which also require the extra computing power provided by the efficient use of multicore CPUs and manycore devices, and analysing larger amounts of data. These two factors have a significant impact on improving the quality of the physics research.

The specialised design of the framework for the specific field of particle physics data analysis allows to implement better automatic parallelization mechanisms than the equivalent general purpose frameworks. On homogeneous platforms, it has been demonstrated in \cite{paperAMP,Msc:AMP} that a single shared or distributed memory parallel implementation may not provide the best efficiency when compared to an hybrid implementation. This framework will attempt to use hybrid parallel configurations in specific cases on a single computing system, while other frameworks assume that shared memory paradigm best suits all applications needs. On heterogeneous platforms, the framework will initially support automatic parallelization for both \nvidia GPU and \intel Xeon Phi devices, with dynamic load balance among CPU and accelerator devices. The framework will use an efficient load balance library for heterogeneous platforms (most libraries only support GPUs), and possible extend its performance model to support the use of \intel Xeon Phi hardware accelerators.

\section{Document Structure}
\label{structure}

This document is structured as follows:

\begin{description}
	\item[Introduction:] this chapter contextualises the work in section \ref{context}, and presents the scientific problem that motivates the PhD thesis proposal in section \ref{motivation}. Section \ref{goals} presents the goals and scientific contribution of this PhD.
	\item[State of the Art:] this chapter presents the current hardware and software technology available for homogeneous and heterogeneous platforms in sections \ref{homo_systems} and \ref{hetero_systems}, respectively. Section \ref{particle_frameworks} contextualises the current work on data analysis optimisation and available libraries for particle physics. Section \ref{tools} presents the available tools and libraries to profile parallel applications.
	\item[An Unified Particle Physics Framework:] the current particle physics skeleton library used by the LIP research group is presented in section \ref{lipminianalysis}. Section \ref{new_framework} presents the conceptual design and preliminary prototypes of the efficient particle physics framework proposed for the PhD thesis.
	\item[Research Plan:] the research plan for the PhD thesis work is presented in this chapter.
\end{description}
